@echo off
REM Development setup script for Reference Chat (Windows)
REM This script helps set up the development environment on Windows

echo.
echo ğŸš€ Setting up Reference Chat development environment...
echo.

REM Check if Python is available
python --version >nul 2>&1
if %errorlevel% neq 0 (
    echo âŒ Python is not installed or not in PATH. Please install Python 3.8+ and try again.
    pause
    exit /b 1
)

echo âœ… Python is available
python --version

REM Create virtual environment
echo.
echo ğŸ“¦ Creating virtual environment...
if not exist venv (
    python -m venv venv
    echo âœ… Virtual environment created successfully
) else (
    echo âš ï¸  Virtual environment already exists
)

REM Activate virtual environment
echo.
echo ğŸ”„ Activating virtual environment...
call venv\Scripts\activate.bat

REM Upgrade pip
echo.
echo ğŸ“ˆ Upgrading pip...
python -m pip install --upgrade pip

REM Install dependencies
echo.
echo ğŸ“¥ Installing Python dependencies...
pip install -r requirements.txt

REM Install development tools
echo.
echo ğŸ› ï¸  Installing development tools...
pip install pytest pytest-asyncio black flake8 mypy

REM Create .env file if it doesn't exist
echo.
if not exist .env (
    echo ğŸ“ Creating .env file from template...
    copy .env.example .env
    echo âš ï¸  Please review and update .env file with your preferences
) else (
    echo âš ï¸  .env file already exists
)

REM Create necessary directories
echo.
echo ğŸ“ Creating necessary directories...
if not exist docs mkdir docs
if not exist logs mkdir logs

REM Download NLTK data
echo.
echo ğŸ“š Downloading NLTK data...
python -c "import nltk; nltk.download('punkt', quiet=True); nltk.download('punkt_tab', quiet=True); print('NLTK data downloaded successfully')"

REM Check if Ollama is available
echo.
echo ğŸ¤– Checking Ollama availability...
where ollama >nul 2>&1
if %errorlevel% equ 0 (
    echo âœ… Ollama is installed
    
    REM Check if Ollama is running
    curl -s http://localhost:11434/api/tags >nul 2>&1
    if %errorlevel% equ 0 (
        echo âœ… Ollama service is running
        echo.
        echo ğŸ“‹ Available Ollama models:
        ollama list
    ) else (
        echo âš ï¸  Ollama is installed but not running. Start it with: ollama serve
    )
) else (
    echo âš ï¸  Ollama is not installed. Please install it from https://ollama.ai/
    echo    After installation, run: ollama pull llama3
)

REM Run tests to verify setup
echo.
echo ğŸ§ª Running tests to verify setup...
python -m pytest tests/ -v --tb=short
if %errorlevel% equ 0 (
    echo âœ… All tests passed! Setup completed successfully.
) else (
    echo âš ï¸  Some tests failed. The setup is mostly complete, but you may need to troubleshoot.
)

REM Summary
echo.
echo ğŸ“‹ Setup Summary:
echo ==================
echo âœ… Python environment: Ready
echo âœ… Dependencies: Installed
echo âœ… Configuration: Created
echo âœ… Directories: Created
echo.
echo ğŸš€ Next steps:
echo 1. Review .env file and adjust settings if needed
echo 2. Ensure Ollama is installed and running
echo 3. Pull an Ollama model: ollama pull llama3
echo 4. Start the backend: python -m uvicorn backend.main:app --reload
echo 5. Start the frontend: streamlit run frontend/app.py
echo.
echo ğŸ“š Documentation: http://localhost:8000/docs (when backend is running)
echo ğŸ¨ Frontend: http://localhost:8501 (when frontend is running)
echo.
echo âœ… Development environment setup complete! ğŸ‰
echo.
pause
